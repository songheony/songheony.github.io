---
layout: post
title: AAA 소개
comments: true
---

## Introduction
### Visual Object Tracking (VOT)

VOT는 아직도 많은 관심을 받고 있는 연구분야이며, 매년 다양한 논문들이 발표되고 있습니다.  
간단하게 테스크를 소개해드리면 동영상에서 원하는 물체를 트래킹하는 문제로,
트래킹하고 싶은 물체의 frame 1에서의 위치와 frame 1의 이미지가 주어지고 이를 바탕으로  
이후의 프레임(frame 2, 3, 4, ...)에서의 이미지가 순차적으로 주어질때 물체의 위치를 예측하는 문제 입니다.  

아직도 많은 연구가 발표되고 있는만큼, 여러가지 문제점이 있는데  
동영상마다 추적해야하는 대상이 너무 다름 (e.g., 개, 물고기, 비닐봉지, 사람, 차 등),  
하나의 동영상에 대해서 다양한 방해 (e.g., target appearance change, target motion change, occlusion, camera motion, environment illumination change) 등이 있습니다.  
이렇게 다양한 문제들이 있기 때문에, 모든 데이터에 대해서 다른 알고리즘보다 압도적으로 좋은 전지전능한 트래커는 없습니다.  

| ![그림1](https://i.imgur.com/bCWj0lO.jpg) | 
|:--:| 
| 그림1 각 벤치마크에 대해서 각 트래커들이 가장 좋았던 동영상의 비율 |

따라서 하나의 트래킹 방법만으로 해결하는것은 불가능하기 때문에,  
복수의 트래킹 방법들을 동시에 사용하는 여러 방법들이 제안되어 있습니다.  
이렇게 복수의 트래킹 방법들을 사용하는것으로 더 robust하게 물체를 트래킹하는것이 목표가 됩니다.  
앞으로는 사용되어지는 복수의 트래킹 방법들을 전문가들이라고 칭하며,  
전문가들을 동시에 사용하는 방법을 aggregation-based method라고 칭합니다.  

### Aggregation-based methods

여러 aggregation-based method들이 현재까지 제안되어 왔으며,  
대부분의 방법들이 단수의 트래킹 알고리즘보다는 데이터에 robust하게 트래킹에 성공하였습니다.  

하지만 이러한 방법들의 가장 큰 문제점은 사용하는 트래킹 알고리즘에 많은 제약을 둔다는 것입니다.  
대표적으로는,  

* 전문가는 재학습이 가능해야 한다[1].
* 전문가는 물체의 위치를 예측하는것만이 아닌, 예측에 사용한 confidence map도 반환해야한다[2].
* 전문가들은 서로 상관이 있어야 한다[3].

등이 있습니다.  

따라서, 전문가들을 선별하는것만으로도 많은 시간과 노력이 필요합니다.

## 제안방법 - AAA

| ![그림2](https://i.imgur.com/iEwQ3Ia.jpg) | 
|:--:| 
| 그림2 제안방법의 개요도 (AAA). 각 전문가의 예측중 하나를, 전문가의 weight(i.e., 신뢰도)에 기반하여, 확률적으로 선택하고 이를 물체의 위치로 반환한다. 앵커 프레임에서는 피드백을 이용하여 전문가의 weight를 업데이트한다. |

이러한 문제점들을 해결하기 위해, 우리들은 임의의 어떠한 트래킹 방법도 전문가로 사용가능한 aggregation-based method, AAA를 제안합니다.  
제안방법은 다음과 같은 장점이 있습니다.

* 임의의 트래킹 방법들을 전문가로써 사용하여 임의의 물체를 추적하여도 성능이 이론적으로 보장되어진다.
* 실험적으로도 많은 벤치마크와 전문가에 대해서 state-of-the-art 의 성능을 얻었다.

제안방법은 머신러닝의 이론필드에서 활발히 연구되고 있는 Online learning 분야를 위해 제안되어진 알고리즘[4]에 기반하고 있으며,  
다음과 같은 방법으로 $N$ 개의 전문가들을 동시에 사용합니다.  

0. 동영상의 물체를 트래킹하기 앞서, 각 전문가에 weight를 부여한다.
1. 각 프레임마다 전문가이 각자 물체의 위치를 예측한다.
2. 전문가들이 예측한 위치중, 물체라고 확신할 수 있는 예측이 있는지 확인한다.
    * 만약 확신할 수 있다면 해당 프레임을 anchor frame으로 판단하고 전문가들의 weight를 업데이트한다.
3. 각 전문가의 신뢰도를 그 전문가의 예측이 선택될 확률로써 사용하여, 예측들중 하나를 확률적으로 선택한다.

### Weight

각 전문가의 weight는 해당 전문가를 얼마나 신뢰할수 있는지, 즉 신뢰도를 나타낸다.
동영상의 첫 프레임에서 물체의 위치와 이미지가 주어지면, 각 전문가의 신뢰도를 ![1/N](https://latex.codecogs.com/gif.latex?1/N)으로 초기화한다.  
전문가들의 신뢰도는 특정한 타이밍에서만 업데이트되어지며, 우리는 그것들 anchor frame이라고 부른다.  

만약 현재 프레임 ![t](https://latex.codecogs.com/gif.latex?t)가 anchor frame이라고 판단되어진다면,  
직전의 anchor frame u의 다음 프레임부터 현재 anchor frame ![t](https://latex.codecogs.com/gif.latex?t)까지의 물체의 위치정보를 담고 있는 피드백 ![feedback](https://latex.codecogs.com/gif.latex?y%5E%7Bu&plus;1%7D%2C%20%5Cldots%2C%20y%5Et)라는것을 만든다.  
피드백과 전문가 ![i](https://latex.codecogs.com/gif.latex?i)가 예측했던 과거의 물체의 위치를 비교하며 전문가 ![i](https://latex.codecogs.com/gif.latex?i)의 손실 ![Loss](https://latex.codecogs.com/gif.latex?L_i)을 계산한다.  

![Loss_equation](https://latex.codecogs.com/gif.latex?L_i%20%3D%20%5Csum_%7B%5Ctau%3Du&plus;1%7D%5Et%201%20-%20%5Ctext%7BGIoU%7D%28f_i%5E%5Ctau%2C%20y%5E%5Ctau%29)  

이후 이 손실에 기반하여 전문가 ![i](https://latex.codecogs.com/gif.latex?i)의 weight ![weight](https://latex.codecogs.com/gif.latex?w_i)를 업데이트한다.  

![Weight_equation](https://latex.codecogs.com/gif.latex?w_i%20%5Cleftarrow%20%5Cfrac%7Bw_i%20%5Cexp%5Cleft%28-%5Ceta%20L_i%5Cright%29%7D%20%7B%5Csum_%7Bj%3D1%7D%5EN%20w_j%20%5Cexp%5Cleft%28-%5Ceta%20L_j%5Cright%29%7D)

### Anchor frame

Anchor frame은 전문가의 weight를 업데이트하는 타이밍으로, 제안방법에서 굉장히 중요한 부분이다.
제안방법에서는 모든 물체에 적용할 수 있으며, 추가 학습과정이 필요없는 방법을 이용하여 anchor frame을 판단한다.  

ImageNet으로 학습되어진 ResNet을 이용하여 물체의 첫 프레임에서의 이미지의 특징벡터를 추출한뒤,  
각 프레임에서 전문가들이 예측한 위치의 특징벡터 또한 추출한다.  
만약 첫 프레임에서의 특징벡터와 예측되어진 특징벡터중 코사인 유사도가 특정값 ![theta](https://latex.codecogs.com/gif.latex?%5Ctheta) 이상인 예측이 존재한다면,  
해당 위치에 물체가 있다고 확신한뒤, 현재 프레임을 anchor frame이라고 판단한다.

![theta](https://latex.codecogs.com/gif.latex?%5Ctheta) 는 제안방법에서 필요로하는 "유일한" 하이퍼 파라미터로써, 제안방법의 성능을 평가하는 벤치마크와는 다른 벤치마크에서 그 값을 조정하였다.

물론 이외에도 다양한 Metric learning기반의 방법들이 사용될 수 있다고 생각한다.  
그러나 이번 연구에서는 되도록 간단하게 판단할 수 있도록 위의 방법을 사용하였다.

### 피드백

피드백은 과거의 물체의 위치정보를 담고 있는것으로써,  
전문가들을 정확히 평가하기 위해 실제 물체의 위치와 비슷해야 한다.  

이번 연구에서는 오프라인 트래킹 방법[5]을 이용하여 피드백을 생성한다.  
오프라인 트래킹 방법은 온라인 트래킹 방법과는 다르게, 트래킹에 필요한 모든 이미지 정보를 받고 나서 트래킹을 행한다.  
따라서 온라인 트래킹 방법에 비해 더 robust하게 물체를 트래킹하는것이 가능하다.  

## 성능의 이론적 보장

Online learning 분야에서는 regret을 최소화하는것을 목표로 한다.  
regret이란것은 선택할 수 있는 가장 최선의 방법과 실제로 선택한 방법의 차이 (i.e., 후회의 정도)를 뜻한다.  

VOT에서 aggregation-based method에서 보자면 regret은 다음과 같이 정의할 수 있다.  
물체를 전부 트래킹하고 난뒤에, 전문가들중 누적손실 (총 트래킹 에러)이 가장 낮은 전문가를 베스트 전문가라고 한다.  
그러면 regret은 베스트 전문가의 누적손실과 제안방법의 누적손실의 차이로써 정의된다.  

![regret](https://latex.codecogs.com/gif.latex?R_T%20%3D%20%5Cmathbb%20E%5Cleft%5B%5Csum_%7Bt%3D1%7D%5ET%20%5Cell%28p%5Et%2C%20y%5Et%29%5Cright%5D%20-%20%5Cmin_%7Bi%3D%201%2C%5Cldots%2C%20N%7D%20%5Csum_%7Bt%3D1%7D%5ET%20%5Cell%28f_i%5Et%2C%20y%5Et%29)  
단 ![loss](https://latex.codecogs.com/gif.latex?%5Cell%28%5Ccdot%29)는 손실함수이며 제안방법에서는 ![loss_proposed](https://latex.codecogs.com/gif.latex?1%20-%20%5Ctext%7BGIoU%7D%28f_i%5E%5Ctau%2C%20y%5E%5Ctau%29)에 해당한다. ![pred](https://latex.codecogs.com/gif.latex?p%5Et)는 와 ![est](https://latex.codecogs.com/gif.latex?f_i%5Et)는 각각 제안방법과 전문가 i가 프레임 t에서 예측한 물체의 위치이다.

이때 제안방법을 이용하는것으로 regret은 전문가와 환경(트래킹 대상, 비디오의 길이 등 모든 정보)에 무관하게  
다음과 같이 upper bound되어 진다.

![proposed_regret](https://latex.codecogs.com/gif.latex?R_T%20%3D%20O%20%5Cleft%28%5Csqrt%7B%5Cleft%28M%5E*%20&plus;%20%7BD%20%5Cover%20T%7DM%5E*%5Cright%29%5Cln%20N%7D%20%5Cright%29)  
단 ![m*](https://latex.codecogs.com/gif.latex?M%5E*)는 베스트 전문가의 누적손실이다. 또한 ![D](https://latex.codecogs.com/gif.latex?D)는 피드백이 딜레이된 정도를 나타내며 총 ![Q](https://latex.codecogs.com/gif.latex?Q)개의 anchor frame의 집합을 ![Q_set](https://latex.codecogs.com/gif.latex?u_1%2C%5Cldots%2Cu_Q) 라고 할때 다음과 같이 계산가능하다.  

![delay](https://latex.codecogs.com/gif.latex?D%3D%5Csum_%7Bq%3D1%7D%5EQ%5Csum_%7Bt%3D1%7D%5E%7Bu_q%20-%20u_%7Bq-1%7D%7Dt)

bound에서 알 수 있듯이, regret은 총 전문가의 수 ![N](https://latex.codecogs.com/gif.latex?N)에 거의 영향을 받지 않는다. 

즉, 제안방법은 어떠한 전문가들을 어떠한 비디오에서 트래킹하는데 사용하건 적어도 최고의 전문가와 비슷한 성능을 보인다는것이 이론적으로 보장된다.

## 실험

우리는 임의의 비디오에 대해서도 제안방법이 좋은 성능을 얻는지 확인하기 위하여 되도록 많은 벤치마크(OTB2015, TColor128, UAV123, NFS, LaSOT, VOT2018)에서 성능을 평가하였다.  

또한 제안방법을 각 전문가의 성능과 비교함은 물론, 다음의 3개의 aggregation-based method와도 비교하였다.

* [3]의 전문가선택 알고리즘 - MCCT
* 매 프레임마다 전문가의 예측중 하나를 랜덤하게 선택 - Random
* 매 프레임마다 첫 프레임에서의 특징벡터와 가장 유사한 예측을 선택 - Max

### 전문가의 성능에 따른 비교

임의의 전문가를 통합하였을때의 성능을 확인하기 위하여 12개의 트래커를 준비하고 다음과 같이 그룹을 나누고 각각의 그룹에 대해서 실험을 진행하였다.

* 상대적으로 성능이 높은 6개의 트래커 (ATOM, DaSiamRPN, SiamMCF, SiamRPN++, SPM, and THOR) - High group
* 상대적으로 성능이 낮은 6개의 트래커 (GradNet, MemTrack, SiamDW, SiamFC, SiamRPN, and Staple) - Low group
* High group에서 3개(ATOM, SiamRPN++, and SPM) Low group에서 3개 (MemTrack, SiamFC, and Staple) - Mix group

먼저 High group에서의 성능은 다음과 같았다.  

| 표1 High group의 전문가들을 통합했을때의 결과 |
|:--:| 
| ![표1](https://i.imgur.com/VQsJWhM.png) | 
| 빨간색으로 표시한것은 가장 성능이 좋은 트래커, 파란색으로 표시한것은 2번째로 성능이 좋은 트래커의 점수이다. |

대부분의 벤치마크에 대해서 모든 전문가보다 좋은 성능을 얻었음을 알 수 있다.  

또한 anchor frame에서 실제로 물체와 비슷한것들을 검출했는지 확인하기 위하여, anchor frame에 대해서만 트래킹 성능을 평가하였다.

| 표2 High group에 대해 anchor frame에서의 결과 |
|:--:| 
| ![표2](https://i.imgur.com/L3nTBU5.png) |

좀 더 자세히 성능을 분석하기 위하여, 각 동영상에서 트래커들에게 순위를 매겼고 각 트래커별로 순위의 빈도를 그래프로 나타내였다.  

| ![그림2](https://i.imgur.com/bk5UTDg.jpg) | 
|:--:| 
| 그림2 High group의 전문가들을 통합했을때의 순위빈도 |

예를들어서, OTB2015에서 제안방법이 2등을 차지한 동영상은 30%있다는것을 알 수 있다. 아래의 그래프는 누적빈도이다.  

이로부터 대부분의 동영상에 대해서 제안방법은 2번째 혹은 3번째로 좋은 성능을 얻은 경우가 많았으며 이는 베스트 전문가와 비슷한 성능을 얻는다는 목표를 달성했음을 알 수 있으며, 이로인해 벤치마크 전체에 대한 평균성능이 모든 전문가보다 좋아진것이다.  

다음은 Low group의 전문가들을 통합하였을때의 결과이다.  

| 표3 Low group의 전문가들을 통합했을때의 결과 |
|:--:| 
| ![표3](https://i.imgur.com/ERHMO8Z.png) |

역시나 대부분의 벤치마크에 대해서 모든 전문가보다 비슷하거나 좋은 성능을 얻었음을 알 수 있다.  

마지막으로 Mix group의 전문가들을 통합하였을때의 결과이다.  

| 표4 Mix group의 전문가들을 통합했을때의 결과 |
|:--:| 
| ![표4](https://i.imgur.com/Gpjl9zW.png) |

High group과 Low group을 통합했을때보다는 좋지 못한 성능을 얻은것을 알 수 있다.  
이를 분석하기 위해, regret과 오프라인 트래커의 성능을 계산하였고 다음과 같았다.  

| 표5 Mix group의 전문가들을 통합했을때의 regret |
|:--:| 
| ![표5](https://i.imgur.com/X4cO8uu.png) |

보는것과 같이 regret 자체는 음수로 regret을 최소화하는데는 충분히 성공하였고, 오히려 모든 전문가보다 좋은 성능을 얻었어야 했다.

| 표6 Mix group의 전문가들을 통합했을때의 오프라인 트래커의 성능 |
|:--:| 
| ![표6](https://i.imgur.com/UtLSeGw.png) |

이는 오프라인 트래커의 성능이 좋지 않아서 이였고, 이번 제안방법에서는 굉장히 간단하게 오프라인 트래커를 만들었기 때문에 이러한 성능이 얻어졌다. 즉 오프라인 트래커의 성능만 좋았다면 제안방법이 Mix group의 전문가들을 통합하였을때도 좋은 성능을 얻을 수 있다고 기대할 수 있다.  

### 단일 트래커를 사용할시의 비교

대부분의 트래킹 방법의 논문들에서는 하나의 알고리즘에 대해서도 여러개의 파라미터(백본 네트워크, NN의 weight, 하이퍼 파라미터 등)을 제안하고 있으며 여러 상황에서 어떠한 파라미터가 좋은지를 제시하고 있다.  

이번 연구에서는 이러한 경우를 위하여 여러 파라미터중 하나를 선택하는것이 아닌 모두를 전문가로써 사용하여 비교하였다.  

| 표7 SiamDW의 전문가들을 통합했을때의 오프라인 트래커의 성능 |
|:--:| 
| ![표7](https://i.imgur.com/wzoNPhD.png) |
| 각 트래커의 이름은 "base algorithm_backbone network(_target benchmark)"를 나타낸다. SiamDW와 SiamRPN의 저자들은 같은 backbone network에 대해서도 벤치마크에 대해서 다른 하이퍼 파라미터를 제공한다. |

| 표8 SiamRPN++의 전문가들을 통합했을때의 오프라인 트래커의 성능 |
|:--:| 
| ![표8](https://i.imgur.com/S4sZlAf.png) |

위의 표들로부터 알 수 있듯이 제안방법이 대부분의 경우에 전문가들보다 좋은 성능을 얻었다. 따라서 모든 파라미터를 전문가로써 사용하는것이 유용하다는것을 확인하였다.  

## 결론

우리는 전문가들을 통합하는 트래킹 방법을 제안하였으며, 그 성능은 regret에 대하여 이론적으로 보장되어 있다. 또한 다양한 전문가들과 벤치마크에서 실험을 진행하여 state-of-the-art를 달성하였다. 

## 인용
[1] Avidan, Shai. "Ensemble tracking." TPAMI 29.2 (2007): 261-271.  
[2] Qi, Yuankai, et al. "Hedged deep tracking." In Proc. CVPR. 2016.  
[3] Wang, Ning, et al. "Multi-cue correlation filters for robust visual tracking." In Proc. CVPR. 2018.  
[4] Quanrud, Kent, and Daniel Khashabi. "Online learning with adversarial delays." In Proc. NIPS. 2015.  
[5] Zhang, Li, Yuan Li, and Ramakant Nevatia. "Global data association for multi-object tracking using network flows." In Proc. CVPR. 2008.
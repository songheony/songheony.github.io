---
layout: post
title: TAkS 소개
comments: true
---

# No Regret Sample Selection with Noisy Labels

Heon Song, Nariaki Mitsuo, Daiki Suehiro, Seiichi Uchida

>Deep neural networks (DNNs) suffer from noisy-labeled data because of the risk of overfitting. To avoid the risk, in this paper, we propose a novel DNN training method with sample selection based on adaptive k-set selection, which selects k (< n) clean sample candidates from the whole n noisy training samples at each epoch. It has a strong advantage of guaranteeing the performance of the selection theoretically. Roughly speaking, a regret, which is defined by the difference between the actual selection and the best selection, of the proposed method is theoretically bounded, even though the best selection is unknown until the end of all epochs. The experimental results on multiple noisy-labeled datasets demonstrate that our sample selection strategy works effectively in the DNN training; in fact, the proposed method achieved the best or the second-best performance among state-of-the-art methods, while requiring a significantly lower computational cost.

arXiv 버전: [https://arxiv.org/abs/2003.03179](https://arxiv.org/abs/2003.03179)  
Github 코드: [https://github.com/songheony/TAkS](https://github.com/songheony/TAkS)

## Introduction

| ![그림1](https://raw.githubusercontent.com/songheony/TAkS/master/assets/Fig1.png) | 
|:--:| 
| 그림1 가장 간단한 k개의 데이터 선택방법 (a)와 제안방법 TA$k$S의 데이터 선택방법 (b).  |

### Learning with noisy labels

현재 사용되고 있는 딥러닝 모델의 경우에는 대부분 overparameterized되었기 때문에, 잘못 라벨링된 데이터에도 오버피팅하는 문제점이 있습니다.  
이를 막기 위하여, 여러 제대로 라벨링된 데이터만을 선택하는 방법론 혹은 학습에 도움이 되는 데이터만을 선택하는 여러 방법들이 제안되어 왔습니다.

이러한 잘못 라벨링된 데이터를 다루는 분야를 learning with noisy labels라고 부르며, 여러 방법들이 제안되어 왔습니다[1].  
제안방법 또한 이러한 learning with noisy labels를 해결하기 위한 방법중 하나로써, 전체 데이터중 $k$개의 제대로 라벨링된 데이터 집합만을 선택하여 학습하는 방법을 제안하고 있습니다. 

$k$개의 데이터를 선택하는 가장 간단한 방법을 그림1에서 보이고 있습니다.  
(a)의 경우, 모든 가능한 $k$개의 데이터를 가지는 부분집합들을 선택하여 해당 부분집합의 갯수만큼의 모델을 준비하고 데이터들을 학습합니다.  
이러한 방법을 사용하면 가장 이상적인 $k$개의 데이터를 선택할 수 있지만, $nCk$개의 수없이 많은 모델들을 학습시켜야 하는 문제점이 있습니다.

따라서 (b)에서 보이는것처럼, 제안방법에서는 단 하나의 모델을 학습하는것만으로도 (a)의 모델들중 가장 이상적인 모델과 비슷한 성능을 가지는 것을 목표로 합니다.

### Related works

여러 learning with noisy labels를 해결하기 위한 방법들이 제안되어 왔으며, 그중에서도 특정 데이터만을 선택하여 학습하는 방법들이 자주 사용되고 있습니다.

가장 대표적이며, 현재 state-of-the-art인 방법론들은 두개의 모델을 동시에 학습시키는 co-training방법을 주로 사용하고 있습니다.  
예를들어, Co-teaching[2]의 경우 두개의 모델을 동시에 학습하면서 하나의 모델에 대해서 손실 (혹은 에러)이 작았던 특정 데이터만을 선택하여 다른 모델을 학습시키는 방식으로 학습을 진행합니다.  
이 외에도, Decouple이나 Co-teaching+ 그리고 최근 CVPR에서 공개된 JoCoR도 비슷한 방법을 사용하고 있습니다.

하지만 이런 방법론들은 다음과 같은 문제점을 가지고 있습니다. 

* 이론적으로 학습능력이 분석되지 않았다.
* 두개의 모델을 동시에 학습해야 하기 때문에, 많은 시간, 메모리 코스트가 발생한다.

## 제안방법 - TA$k$S

제안방법에서는 매 에폭마다 아래의 굉장히 간단한 방법을 반복합니다.

1. 각 데이터의 noisiness에 기반하여 $k$개의 데이터를 선택후 학습합니다.
2. 각 데이터의 noisiness를 갱신합니다.

이러한 간단한 방법으로 제안방법은 최적의 $k$-set을 선택하여 학습했을때와 비슷한 성능을 얻는것이 이론적으로 보장되어 있습니다.

### Noisiness

제안방법에서는 각 데이터 $(x_i, y_i)$에 대해서 잘못 라벨링된 정도를 표현하기 위하여 noisiness를 다음과 같이 계산합니다.

$$ \ell_{i,t} = \frac{1- \mathrm{is}(f_t(x_i)= y_i)p(f_t(x_i))}{2} $$

이때 $f_t$는 t에폭에서의 모델을 나타내며, 함수 $\mathrm{is}(\cdot)$는 $\cdot$이 참일 경우 1을 반환하고 거짓일 경우 -1을 반환하는 함수를 나타냅니다.  
즉, noisiness는 확신을 가지고 정답을 맞추었을 경우에 0으로 가장 낮으며, 확신을 가지고 틀렸을 경우에 1로 가장 높습니다.

중요한것은, 어떤 임의의 함수도 noisiness을 계산하는 함수로써 사용할 수 있다는것입니다. 함수의 범위가 유한한 값으로 제한된다면 어떤 임의의 noisiness를 사용하더라도 여전히 제안방법의 성능은 해당 함수에 따른 성능이 이론적으로 보장됩니다.

### FPL을 이용한 $k$-set 선택

noisiness를 계산했다더라도, 그림1의 (a)에서 본것처럼 여전히 모든 조합(부분집합)들을 시험해보지 않고서는 어떤 부분집합을 사용해야지 "학습이 끝났을때" 좋은 성능을 얻을 수 있는지 알 수는 없습니다.

그러나 Follow the Perturbed Leader (FPL)을 사용하여 학습할 데이터의 부분집합을 선택하면 제안방법의 성능이 이론적으로 보장되게 됩니다[3].  
또한 FPL은 굉장히 빠른 속도($O(n)$)로 데이터를 선택하는것이 가능합니다.  

FPL은 Follow the Leader (FTL)의 개선된 알고리즘으로, perturbed loss를 사용하는것으로 특정 $k$개의 데이터에 오버피팅되지 않고 더 좋은 성능을 내는 것으로 알려져 있습니다.  
FPL에서는 다음과 같은 목적함수를 최소화 하는것을 목표로 합니다.

$$\left(\sum^{t}_{\tau=1} \mathbf{d} \cdot \boldsymbol{\ell}_\tau + \eta\mathbf{d} \cdot \boldsymbol{r}  \right)=\left(\sum^{t}_{\tau=1} \boldsymbol{\ell}_\tau + \eta\boldsymbol{r}\right)\cdot\mathbf{d}$$

단 이때, $\boldsymbol{\ell}_\tau$는 $\tau$ 에폭에서의 모든 데이터에 대한 noisiness를 나타내는 벡터이며, $\boldsymbol{r}$는 $\mathcal{N}(0,I)$의 랜덤 벡터를 나타냅니다.  
또한 $\mathbf{d}$는 $k$-hot 벡터로 선택할 데이터를 1로 나타냅니다.  

따라서 $$\left(\sum^{t}_{\tau=1} \boldsymbol{\ell}_\tau + \eta\boldsymbol{r}\right)$$ 가 작은 bottom-k를 선택하는 문제로 바뀌게 되며, 이는 $O(n)$의 계산시간을 필요로 합니다.

## 성능의 이론적 보장

$k$-set 선택 문제에서는 Regret이라 불리는 다음의 값을 최소화하는것을 목표로 합니다.

$$R_T = \sum_{t=1}^T \mathbf{d}_t \cdot \boldsymbol{\ell}_t - \min_{\mathbf{d} \in \mathcal{D}} \sum_{t=1}^T \mathbf{d} \cdot \boldsymbol{\ell}_t$$

이때, $\mathcal{D}$는 모든 가능한 조합을 나타냅니다.  
이 식으로부터 regret이라고 하는것은 최적의 $k$-set으로 모델을 학습시켰을때와 제안방법이 선택해온 $k$-set으로 모델을 학습시켰을때의 noisiness의 차이, 즉 특정 $k$-set을 썻어야 했다는 후회를 줄이는것을 목표로 합니다.

이때, 제안방법을 사용하는 것으로 모든 임의의 데이터와 모델에 대해서 다음의 성립합니다.

$$R_T \leq 2\sqrt{2kT \ln \binom{n}{k}}$$

이는 제안방법의 regret이 상한을 가짐을 나타내며, 상한이 모든 조합의 경우의 수 $\binom{n}{k}$에 의해 폭발적으로 늘어나는 것이 아닌 $\sqrt{\ln \binom{n}{k}}$에 비례하며 비교적 적은 영향만을 받는것을 의미합니다.

## 실험

### 데이터셋

제안방법의 성능을 확인하기 위하여 다음과 같은 데이터셋을 이용하여 실험을 진행하였습니다: MNIST, CIFAR-10, CIFAR-100, 그리고 Clothing1M.

단, MNIST, CIFAR-10, CIFAR-100의 데이터셋에는 잘못 라벨링된 데이터가 없었기 때문에 다음과 같은 두가지 노이즈를 추가하였습니다.

* Symmetric noise: 데이터중 일부(20%, 50%, 혹은 80%)가 랜덤하게 다른 클래스로 잘못 라벨링되는 노이즈
* Asymmetric noise: 데이터중 특정 클래스의 일부(40%)가 비슷한 클래스로 잘못 라벨링되는 노이즈 (e.g., 개 $\to$ 고양이)

### 평가기준

학습이 끝난후, 다음과 같은 세가지 평가기준으로 모델을 평가하였습니다.

* Test accuracy: 노이즈가 없는 테스트 데이터에 대한 정확도
* Label precision: 학습에 사용한 데이터중 잘못 라벨링된 데이터의 비율
* Training time: 전체 데이터를 학습했을때 대비 학습에 소요된 시간의 비율

모든 평가기준은 각 에폭마다 측정하였습니다.

### 비교방법

다음과 같은 learning with noisy labels 방법론들과 비교하였습니다.

* F-correction
* Decouple
* Co-teaching
* Co-teaching+
* JoCoR

또한, 모든 학습데이터를 학습하는 기본적인 학습방법을 Standard라고 명명하였습니다.

### 결과

|                 | Standard | F-correction | Decouple |               |          Co-teaching          |           |          Co-teaching+         |           |              JoCoR             |           |          Ours          |               |
|:---------------:|:--------:|:------------:|:--------:|:-------------:|:-----------------------------:|:---------:|:-----------------------------:|:---------:|:------------------------------:|:---------:|:-----------------------------:|:-------------:|
|                 |    Acc   |      Acc     |    Acc   |   xT   |              Acc              | xT |              Acc              | xT |               Acc              | xT |              Acc              |   xT   |
| **MNIST**           |          |              |          |               |                               |           |                               |           |                                |           |                               |               |
|  Symmetric-20% |   78.67  |     90.71    |   94.74  | **1.10** |             94.52             |    1.60   |             97.77             |    1.61   |  :two: **97.88** |    1.47   |  :one: **97.94** |      1.19     |
|  Symmetric-50% |   51.22  |     77.36    |   66.77  |      1.46     |             89.50             |    1.59   |             95.67             |    1.67   |  :two: **95.90** |    1.49   |  :one: **97.17** | **0.97** |
|  Symmetric-80% |   22.43  |     51.16    |   27.42  |      1.50     |             78.52             |    1.59   |             66.13             |    1.73   |  :two: **88.53** |    1.49   |  :one: **92.32** | **0.81** |
| Asymmetric-40% |   78.97  |     88.99    |   82.04  |      1.35     |             90.21             |    1.59   |             92.48             |    1.65   |  :two: **93.91** |    1.50   |  :one: **95.77** | **1.22** |
| **CIFAR-10**        |          |              |          |               |                               |           |                               |           |                                |           |                               |               |
|  Symmetric-20% |   68.92  |     74.21    |   69.95  |      1.61     |             78.16             |    1.98   |             78.68             |    2.00   |  :one: **85.75**  |    1.73   | :two: **83.90** | **0.99** |
|  Symmetric-50% |   41.93  |     52.68    |   40.91  |      1.71     |             70.79             |    1.97   |             56.90             |    1.99   |  :one: **78.92**  |    1.73   | :two: **76.83** | **0.74** |
|  Symmetric-80% |   15.85  |     18.99    |   15.29  |      1.82     | :two: **26.54** |    1.98   |             23.50             |    2.00   |              25.51             |    1.73   |  :one: **40.24** | **0.53** |
| Asymmetric-40% |   69.23  |     69.64    |   69.10  |      1.51     | :two: **73.59** |    1.99   |             68.45             |    2.00   |  :one: **76.13**  |    1.74   |             73.43             | **1.04** |
| **CIFAR-100**       |          |              |          |               |                               |           |                               |           |                                |           |                               |               |
|  Symmetric-20% |   35.51  |     36.04    |   33.82  |      1.75     |             44.03             |    1.97   |             49.24             |    1.99   |  :one: **53.10**  |    1.73   | :two: **50.74** | **0.93** |
|  Symmetric-50% |   17.31  |     21.14    |   15.81  |      1.82     |             34.96             |    1.97   |             40.26             |    2.00   |  :one: **43.28**  |    1.73   | :two: **40.98** | **0.68** |
|  Symmetric-80% |   4.25   |     7.48     |   4.03   |      1.90     | :two: **14.81** |    1.97   |             13.99             |    2.00   |              12.90             |    1.72   |  :one: **16.03** | **0.52** |
| Asymmetric-40% |   27.91  |     27.11    |   26.95  |      1.75     |             28.69             |    1.98   | :two: **34.30** |    2.01   |              32.39             |    1.74   |  :one: **35.23** | **0.98** |
| **Clothing1M**      |          |              |          |               |                               |           |                               |           |                                |           |                               |               |
|       Best      |   67.62  |     67.34    |   68.32  |      1.97     |             68.37             |    1.99   |             68.51             |    1.99   |  :one: **70.30** |    2.00   | :two: **70.28** |      **0.78**     |
|       Last      |   66.05  |     66.73    |   67.69  |               |             68.12             |           |             68.51             |           | :two: **69.79** |           |  :one: **70.28** |               |

Test accuracy와 Training Time을 위의 표에 나타내었습니다.  
제안방법이 대부분의 데이터셋에서 최고 혹은 두번째로 좋은 Test accuracy를 얻었음을 알 수 있습니다.  
특히, 노이즈가 많은 Symmetric-80%에서 다른 방법들에 비해 좋은 성능을 얻었음을 알 수 있습니다.  
이것은 이론적 성능 보장이 **그 어떤 경우에서도 성립**하기 때문임을 알 수 있습니다.  

또한, 학습 시간이 대부분의 경우에서 다른 방법들은 물론 Standard에 비해서도 빠른것을 알 수 있습니다.  
이는 제안방법은 오직 한개의 모델로 학습전에 정한 $k$개의 데이터만을 학습하기 때문입니다.


|     | Decouple |       |       | Co-teaching |                               |                               | Co-teaching+ |       |       |             JoCoR             |                               |                               |          Ours         |                               |                              |
|:---:|:--------:|:-----:|:-----:|:-----------:|:-----------------------------:|:-----------------------------:|:------------:|:-----:|:-----:|:-----------------------------:|:-----------------------------:|:-----------------------------:|:----------------------------:|:-----------------------------:|:----------------------------:|
|     |     MNIST    |  CIFAR-10  |  CIFAR-100 |      MNIST      |              CIFAR-10              |              CIFAR-100             |       MNIST      |  CIFAR-10  |  CIFAR-100 |               MNIST               |              CIFAR-10              |              CIFAR-100             |               MNIST              |              CIFAR-10              |             CIFAR-100             |
| Symmetric-20% |   37.31  | 68.19 | 72.50 |    95.38    |             92.20             |             91.87             |     79.93    | 56.95 | 48.73 | :two: **98.14** | :two: **96.83** | :two: **95.91** | :one: **99.72** |  :one: **98.16** | :one: **97.65** |
| Symmetric-50% |   31.83  | 40.44 | 43.37 |    89.78    |             82.65             |             80.11             |     49.32    |  5.64 | 25.34 | :two: **95.43** | :two: **91.04** | :two: **87.40** | :one: **99.66** |  :one: **93.16** | :one: **91.24** |
| Symmetric-80% |   16.91  | 18.19 | 18.24 |    77.48    | :two: **36.41** | :two: **48.71** |     12.08    | 19.73 | 18.83 | :two: **88.19** |             35.95             |             45.15             | :one: **97.19** |  :one: **51.43** | :one: **51.88** |
| Asymmetric-40% |   52.65  | 68.57 | 56.91 |    93.52    |             86.79             |             62.15             |     79.97    | 79.58 | 36.87 | :two: **95.99** |  :one: **87.94** | :two: **63.59** | :one: **99.11** | :two: **87.47** | :one: **68.41** |

Label precision을 위의 표에 나타내었습니다.  
거의 대부분의 경우에서 가장 높은 Label precision을 얻었음은 물론,  
Test accuracy와 마찬가지로 Symmetric-80%의 경우에 다른 방법들에 비해서 월등히 높은 Label precision을 얻었음을 알 수 있습니다.  
이로써 제대로 라벨링된 데이터만을 선택한다는 목적을 달성하였음을 알 수 있습니다.

## 결론

우리는 잘못 라벨링된 데이터를 포함한 학습데이터를 이용하여 딥러닝 모델을 학습하는 방법을 제안하였습니다.  
제안방법의 성능은 이론적으로 보장되어 있을뿐만 아니라, 실험적으로도 유의미한 성능을 얻었음을 확인하였습니다.

## 인용
[1] B. Frénay and M. Verleysen, “Classification in the presence of label noise: a survey,” TNNLS, vol. 25, no. 5, pp. 845–869, 2013.  
[2] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, and M. Sugiyama, “Co-teaching: Robust training of deep neural networks with extremely noisy labels,” in Proc. NeurIPS, 2018.  
[3] A. Cohen and T. Hazan, “Following  the  perturbed leader for online structured learning,” in Proc. ICML,2015.  
---
layout: post
title: TAkS 소개
comments: true
---

## Introduction

| ![그림1](https://raw.githubusercontent.com/songheony/TAkS/master/assets/Fig1.png) | 
|:--:| 
| 그림1 가장 간단한 k개의 데이터 선택방법 (a)와 제안방법 TA$k$S의 데이터 선택방법 (b).  |

### Learning with noisy labels

현재 사용되고 있는 딥러닝 모델의 경우에는 대부분 overparameterized되었기 때문에, 잘못 라벨링된 데이터에도 오버피팅하는 문제점이 있습니다.  
이를 막기 위하여, 여러 제대로 라벨링된 데이터만을 선택하는 방법론 혹은 학습에 도움이 되는 데이터만을 선택하는 여러 방법들이 제안되어 왔습니다.

이러한 잘못 라벨링된 데이터를 다루는 분야를 learning with noisy labels라고 부르며, 여러 방법들이 제안되어 왔습니다.  
제안방법 또한 이러한 learning with noisy labels를 해결하기 위한 방법중 하나로써, 전체 데이터중 $k$개의 제대로 라벨링된 데이터 집합만을 선택하여 학습하는 방법을 제안하고 있습니다. 

$k$-set의 가장 간단한 방법을 그림1에서 보이고 있습니다.  
(a)의 경우, 모든 가능한 $k$개의 데이터를 가지는 부분집합들을 선택하여 해당 부분집합의 갯수만큼의 모델을 준비하고 데이터들을 학습합니다.  
이러한 방법을 사용하면 가장 이상적인 $k$개의 데이터를 선택할 수 있지만, $nCk$개의 수없이 많은 모델들을 학습시켜야 하는 문제점이 있습니다.

따라서 (b)에서 보이는것처럼, 제안방법에서는 단 하나의 모델을 학습하는것만으로도 (a)의 모델들중 가장 이상적인 모델과 비슷한 성능을 가지는 것을 목표로 합니다.

### Related works

여러 learning with noisy labels를 해결하기 위한 방법들이 제안되어 왔으며, 그중에서도 특정 데이터만을 선택하여 학습하는 방법들이 자주 사용되고 있습니다.

가장 대표적이며, 현재 state-of-the-art인 방법론들은 두개의 모델을 동시에 학습시키는 co-training방법을 주로 사용하고 있습니다.  
예를들어, Co-teaching[0]의 경우 두개의 모델을 동시에 학습하면서 하나의 모델에 대해서 손실 (혹은 에러)이 작았던 특정 데이터만을 선택하여 다른 모델을 학습시키는 방식으로 학습을 진행합니다.  
이 외에도, Decouple이나 Co-teaching+ 그리고 최근 CVPR에서 공개된 JoCoR도 비슷한 방법을 사용하고 있습니다.

하지만 이런 방법론들은 다음과 같은 문제점을 가지고 있습니다. 

* 이론적으로 학습능력이 분석되지 않았다.
* 두개의 모델을 동시에 학습해야 하기 때문에, 많은 시간, 메모리 코스트가 발생한다.

## 제안방법 - TA$k$S

제안방법에서는 매 에폭마다 아래의 굉장히 간단한 방법을 반복합니다.

1. 각 데이터의 noisiness에 기반하여 $k$개의 데이터를 선택후 학습합니다.
2. 각 데이터의 noisiness를 갱신합니다.

이러한 간단한 방법으로 제안방법은 최적의 $k$-set을 선택하여 학습했을때와 비슷한 성능을 얻는것이 이론적으로 보장되어 있습니다.

### Noisiness

제안방법에서는 각 데이터 $(x_i, y_i)$에 대해서 잘못 라벨링된 정도를 표현하기 위하여 noisiness를 다음과 같이 계산합니다.

$$ \ell_{i,t} = \frac{1- \mathrm{is}(f_t(x_i)= y_i)p(f_t(x_i))}{2} $$

이때 $f_t$는 t에폭에서의 모델을 나타내며, 함수 $\mathrm{is}(\cdot)$는 $\cdot$이 참일 경우 1을 반환하고 거짓일 경우 -1을 반환하는 함수를 나타냅니다.  
즉, noisiness는 확신을 가지고 정답을 맞추었을 경우에 0으로 가장 낮으며, 확신을 가지고 틀렸을 경우에 1로 가장 높습니다.

중요한것은, 어떤 임의의 함수도 noisiness을 계산하는 함수로써 사용할 수 있다는것입니다. 함수의 범위가 유한한 값으로 제한된다면 어떤 임의의 noisiness를 사용하더라도 여전히 제안방법의 성능은 해당 함수에 따른 성능이 이론적으로 보장됩니다.

### FPL을 이용한 $k$-set 선택

noisiness를 계산했다더라도, 그림1의 (a)에서 본것처럼 여전히 모든 조합(부분집합)들을 시험해보지 않고서는 어떤 부분집합을 사용해야지 "학습이 끝났을때" 좋은 성능을 얻을 수 있는지 알 수는 없습니다.

그러나 Follow the Perturbed Leader (FPL)을 사용하여 학습할 데이터의 부분집합을 선택하면 제안방법의 성능이 이론적으로 보장되게 됩니다.  
또한 FPL은 굉장히 빠른 속도($O(n)$)로 데이터를 선택하는것이 가능합니다.  

FPL은 Follow the Leader (FTL)의 개선된 알고리즘으로, perturbed loss를 사용하는것으로 특정 $k$개의 데이터에 오버피팅되지 않고 더 좋은 성능을 내는 것으로 알려져 있습니다.  
FPL에서는 다음과 같은 목적함수를 최소화 하는것을 목표로 합니다.

$$\left(\sum^{t}_{\tau=1} \mathbf{b} \cdot \bell_\tau + \frac{1}{\eta}\mathbf{b} \cdot \boldsymbol{r}  \right)=\left(\sum^{t}_{\tau=1} \bell_\tau + \frac{1}{\eta}\boldsymbol{r}\right)\cdot\mathbf{b}$$

단 이때, $\bell_\tau$는 $\tau$ 에폭에서의 모든 데이터에 대한 noisiness를 나타내는 벡터이며, $\boldsymbol{r}$는 $\mathcal{N}(0,I)$의 랜덤 벡터를 나타냅니다.  
또한 $\mathbf{b}$는 $k$-hot 벡터로 선택할 데이터를 1로 나타냅니다.  

따라서 $$\left(\sum^{t}_{\tau=1} \bell_\tau + \frac{1}{\eta}\boldsymbol{r}\right)$$ 가 작은 bottom-k를 선택하는 문제로 바뀌게 되며, 이는 $O(n)$의 계산시간을 필요로 합니다.

## 성능의 이론적 보장

$k$-set 선택 문제에서는 Regret이라 불리는 다음의 값을 최소화하는것을 목표로 합니다.

$$R_T = \sum_{t=1}^T \mathbf{b}_t \cdot \bell_t - \min_{\mathbf{b} \in \mathcal{D}} \sum_{t=1}^T \mathbf{b} \cdot \bell_t$$

이때, $\mathcal{D}$는 모든 가능한 조합을 나타냅니다.  
이 식으로부터 regret이라고 하는것은 최적의 $k$-set으로 모델을 학습시켰을때와 제안방법이 선택해온 $k$-set으로 모델을 학습시켰을때의 noisiness의 차이, 즉 특정 $k$-set을 썻어야 했다는 후회를 줄이는것을 목표로 합니다.

이때, 제안방법을 사용하는 것으로 모든 임의의 데이터와 모델에 대해서 다음의 성립합니다.

$R_T \leq 2\sqrt{2kT \ln \binom{n}{k}}$

이는 제안방법의 regret이 상한을 가짐을 나타내며, 상한이 모든 조합의 경우의 수 $\binom{n}{k}$에 의해 폭발적으로 늘어나는 것이 아닌 $\sqrt{\ln \binom{n}{k}}$에 비례하며 비교적 적은 영향만을 받는것을 의미합니다.

## 실험

## 결론

## 인용